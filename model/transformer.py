import torch
import torch.nn as nn


class MultiHeadAttention(nn.Module):
    def __int__(self):
        super().__init__()

    def forward(self):
        pass


class FeedForward(nn.Module):
    def __int__(self):
        super().__init__()

    def forward(self):
        pass


class Transformer(nn.Module):
    def __int__(self):
        super().__init__()

    def forward(self):
        pass
